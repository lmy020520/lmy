#å®Œæ•´ç‰ˆ
import gradio as gr
import pandas as pd
import json
import re
import os
import docx
import requests
import numpy as np
from typing import List, Dict
from sentence_transformers import SentenceTransformer, util
import torch
from pathlib import Path
import zipfile
import shutil

# é…ç½®éƒ¨åˆ†
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEEPSEEK_API_KEY = "sk-883d825876464ab6966616a3ae887953"
KNOWLEDGE_BASE_PATH = "/home/dockeruser/lmy/äºŒçº§åŒ»ç–—å™¨æ¢°/uft82.csv"
KNOWLEDGE_BASE_PATH_IVD = "/home/dockeruser/lmy/äºŒçº§åŒ»ç–—å™¨æ¢°/æ•°æ®2.csv"
LOCAL_MODEL_PATH = "/home/dockeruser/lmy/Model/model1"
EMBEDDING_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

REQUIRED_FOLDER_CONTENTS = {
    "1.ç›‘ç®¡ä¿¡æ¯": [
        "1.1ç« èŠ‚ç›®å½•.docx",
        "1.2ç”³è¯·è¡¨.docx",
        "1.3æœ¯è¯­ã€ç¼©å†™è¯åˆ—è¡¨.docx",
        "1.4äº§å“åˆ—è¡¨.docx",
        "1.5å…³è”æ–‡ä»¶.docx",
        "1.6ç”³æŠ¥å‰ä¸ç›‘ç®¡æœºæ„çš„è”ç³»æƒ…å†µå’Œæ²Ÿé€šè®°å½•.docx",
        "1.7ç¬¦åˆæ€§å£°æ˜.docx"
    ],
    # å…¶ä»–æ–‡ä»¶å¤¹åŠæ–‡ä»¶è¦æ±‚ä¸ä¹‹å‰ä¸€è‡´
}

class MedicalDeviceClassifier:
    def __init__(self):
        self.md_knowledge_base = None
        self.ivd_knowledge_base = None
        self.embedding_model = None
        self.md_embeddings = None
        self.ivd_embeddings = None
        self.initialize_knowledge_bases()

    def initialize_knowledge_bases(self):
        try:
            # åŠ è½½åŒ»ç–—å™¨æ¢°çŸ¥è¯†åº“
            if not os.path.exists(KNOWLEDGE_BASE_PATH):
                raise FileNotFoundError(f"åŒ»ç–—å™¨æ¢°çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {KNOWLEDGE_BASE_PATH}")
            md_df = pd.read_csv(KNOWLEDGE_BASE_PATH)
            md_required_columns = ['row_id', 'desc', 'intended_use', 'name', 'grade']
            if not all(col in md_df.columns for col in md_required_columns):
                raise ValueError("åŒ»ç–—å™¨æ¢°CSVç¼ºå°‘å¿…è¦åˆ—")
            self.md_knowledge_base = md_df.to_dict('records')
            
            # åŠ è½½IVDçŸ¥è¯†åº“
            if not os.path.exists(KNOWLEDGE_BASE_PATH_IVD):
                raise FileNotFoundError(f"IVDçŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {KNOWLEDGE_BASE_PATH_IVD}")
            ivd_df = pd.read_csv(KNOWLEDGE_BASE_PATH_IVD)
            ivd_required_columns = ['row_id', 'intended_use', 'grade']
            if not all(col in ivd_df.columns for col in ivd_required_columns):
                raise ValueError("IVD CSVç¼ºå°‘å¿…è¦åˆ—")
            self.ivd_knowledge_base = ivd_df.to_dict('records')
            
            # åŠ è½½åµŒå…¥æ¨¡å‹
            print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹({LOCAL_MODEL_PATH})ï¼Œè®¾å¤‡: {EMBEDDING_DEVICE}...")
            if not os.path.exists(LOCAL_MODEL_PATH):
                raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {LOCAL_MODEL_PATH}")
            self.embedding_model = SentenceTransformer(
                model_name_or_path=LOCAL_MODEL_PATH,
                device=EMBEDDING_DEVICE
            )
            
            # ä¸ºä¸¤ä¸ªçŸ¥è¯†åº“ç”ŸæˆåµŒå…¥å‘é‡
            print("æ­£åœ¨ç”ŸæˆåŒ»ç–—å™¨æ¢°çŸ¥è¯†åº“åµŒå…¥å‘é‡...")
            md_corpus = [
                f"{row['name']}ã€‚{row['desc']}ã€‚ç”¨äº{row['intended_use']}" 
                for row in self.md_knowledge_base
            ]
            self.md_embeddings = self.embedding_model.encode(
                md_corpus, 
                show_progress_bar=True,
                convert_to_tensor=True,
                normalize_embeddings=True,
                batch_size=32
            )
            
            print("æ­£åœ¨ç”ŸæˆIVDçŸ¥è¯†åº“åµŒå…¥å‘é‡...")
            ivd_corpus = [
                f"é¢„æœŸç”¨é€”:{row['intended_use']}" 
                for row in self.ivd_knowledge_base
            ]
            self.ivd_embeddings = self.embedding_model.encode(
                ivd_corpus,
                show_progress_bar=True,
                convert_to_tensor=True,
                normalize_embeddings=True,
                batch_size=32
            )
            
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸï¼ŒåŠ è½½{len(self.md_knowledge_base)}æ¡åŒ»ç–—å™¨æ¢°è®°å½•å’Œ{len(self.ivd_knowledge_base)}æ¡IVDè®°å½•")
            
        except Exception as e:
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def call_deepseek_api(self, prompt: str) -> str:
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—å™¨æ¢°/ä½“å¤–è¯Šæ–­è¯•å‰‚åˆ†ç±»ä¸“å®¶ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
1. å¿…é¡»æ˜ç¡®ç»™å‡ºåˆ†ç±»ç­‰çº§ï¼ˆIç±»/IIç±»/IIIç±»ï¼‰
2. å¿…é¡»åŸºäºæä¾›çš„çŸ¥è¯†åº“æ¡ç›®è¿›è¡Œåˆ¤æ–­
3. æœ€ç»ˆå›ç­”å¿…é¡»åŒ…å«ä»¥ä¸‹JSONç»“æ„ï¼š
```json
{
  "classification": "I/II/III",
  "confidence": 0.0-1.0,
  "rationale": "åˆ†ç±»ä¾æ®è¯´æ˜",
  "matched_id": åŒ¹é…çš„æ¡ç›®ID,
  "missing_info": ["éœ€è¦è¡¥å……çš„å­—æ®µ"]
}
```"""
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.1,
            "max_tokens": 800
        }
        
        try:
            response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
        except KeyError:
            raise RuntimeError("APIè¿”å›æ ¼å¼å¼‚å¸¸")

    def parse_document(self, file_path: str) -> str:
        try:
            doc = docx.Document(file_path)
            full_text = []
            current_line = ""
            
            for para in doc.paragraphs:
                text = para.text.strip()
                
                if not text:
                    if current_line:
                        full_text.append(current_line)
                        current_line = ""
                    continue
                    
                if text.endswith(('ã€‚', 'ï¼›', 'ï¼', 'ï¼Ÿ', 'ï¼‰', 'ã€', '.', ';', '!', '?')):
                    if current_line:
                        full_text.append(current_line + text)
                        current_line = ""
                    else:
                        full_text.append(text)
                else:
                    current_line += text
            
            if current_line:
                full_text.append(current_line)
                
            return "\n".join(full_text)
        except Exception as e:
            raise ValueError(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")

    def extract_fields(self, text: str, is_ivd: bool = False) -> dict:
        text = re.sub(r'[:ï¼š]\s*', ': ', text)
        
        patterns = {
            "desc": [
                r"äº§å“æè¿°[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ã€äº§å“æè¿°ã€‘(.+?)(?=ã€|$)",
                r"æè¿°[:ï¼š]\s*(.+?)(?=\n|$)",
                r"(?<=äº§å“æ¦‚è¿°[:ï¼š]).+?(?=\n|$)"
            ],
            "intended_use": [
                r"é¢„æœŸç”¨é€”[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ã€é¢„æœŸç”¨é€”ã€‘(.+?)(?=ã€|$)",
                r"ç”¨é€”[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ç”¨äº(.+?)(?=\n|$)",
                r"é€‚ç”¨èŒƒå›´[:ï¼š]\s*(.+?)(?=\n|$)"
            ],
            "name": [
                r"å“å[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ã€äº§å“åç§°ã€‘(.+?)(?=ã€|$)",
                r"åç§°[:ï¼š]\s*(.+?)(?=\n|$)",
                r"äº§å“å[:ï¼š]\s*(.+?)(?=\n|$)",
                r"æ³¨å†Œåç§°[:ï¼š]\s*(.+?)(?=\n|$)"
            ]
        }
        
        result = {}
        for field, field_patterns in patterns.items():
            for pattern in field_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    result[field] = match.group(1).strip()
                    break
            else:
                result[field] = ""
        
        # éªŒè¯å­—æ®µå®Œæ•´æ€§
        if is_ivd:
            required_fields = ["intended_use"]
            missing = [k for k in required_fields if not result.get(k)]
            if missing:
                raise ValueError(f"ä½“å¤–è¯Šæ–­è¯•å‰‚åˆ†ç±»å¿…é¡»æä¾›é¢„æœŸç”¨é€”")
        else:
            required_fields = ["desc", "intended_use", "name"]
            missing = [k for k in required_fields if not result.get(k)]
            if missing:
                raise ValueError(f"æ–‡æ¡£ä¸­ç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing)}")
        
        return {k: v.strip() for k, v in result.items()}

    def retrieve_candidates(self, query: dict, is_ivd: bool = False, top_k: int = 3) -> List[Dict]:
        if is_ivd:
            if not self.ivd_knowledge_base or self.ivd_embeddings is None:
                raise RuntimeError("IVDçŸ¥è¯†åº“æœªåˆå§‹åŒ–")
            knowledge_base = self.ivd_knowledge_base
            embeddings = self.ivd_embeddings
            query_text = f"é¢„æœŸç”¨é€”:{query['intended_use']}"
        else:
            if not self.md_knowledge_base or self.md_embeddings is None:
                raise RuntimeError("åŒ»ç–—å™¨æ¢°çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
            knowledge_base = self.md_knowledge_base
            embeddings = self.md_embeddings
            query_text = f"{query['name']}ã€‚{query['desc']}ã€‚ç”¨äº{query['intended_use']}"
        
        query_embedding = self.embedding_model.encode(
            query_text, 
            convert_to_tensor=True,
            normalize_embeddings=True
        )
        
        cos_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]
        top_results = torch.topk(cos_scores, k=top_k)
        
        results = []
        for score, idx in zip(top_results[0], top_results[1]):
            row = knowledge_base[idx]
            results.append({
                "row_id": row["row_id"],
                "intended_use": row["intended_use"],
                "grade": row["grade"],
                "similarity": float(score)
            })
        
        return results

    def generate_prompt(self, device_info: dict, candidates: list, is_ivd: bool) -> str:
        candidate_str = "\n".join(
            f"ã€æ¡ç›®#{c['row_id']}ã€‘\n"
            f"é¢„æœŸç”¨é€”: {c['intended_use']}\n" 
            f"ç±»åˆ«: {c['grade']}ç±»\n"
            f"è¯­ä¹‰ç›¸ä¼¼åº¦: {c['similarity']:.2f}\n"
            for c in candidates
        )
        
        device_type = "ä½“å¤–è¯Šæ–­è¯•å‰‚" if is_ivd else "åŒ»ç–—å™¨æ¢°"
        
        return f"""è¯·å¯¹ä»¥ä¸‹{device_type}è¿›è¡Œåˆ†ç±»åˆ†æï¼š

===== å¾…åˆ†ç±»è®¾å¤‡ =====
é¢„æœŸç”¨é€”: {device_info['intended_use']}
åç§°: {device_info.get('name', 'æœªæä¾›')}

===== çŸ¥è¯†åº“å€™é€‰æ¡ç›® =====
{candidate_str}

è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¿”å›JSONæ ¼å¼ç»“æœï¼Œå¿…é¡»åŒ…å«ï¼š
1. æ˜ç¡®çš„åˆ†ç±»ç­‰çº§ï¼ˆI/II/IIIï¼‰
2. ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
3. è¯¦ç»†çš„åˆ†ç±»ä¾æ®
4. åŒ¹é…çš„æ¡ç›®IDï¼ˆå¦‚æ— å¯å†™-1ï¼‰
5. éœ€è¦è¡¥å……çš„ä¿¡æ¯ï¼ˆå¦‚æ— å¯å†™ç©ºåˆ—è¡¨ï¼‰"""

    def process_document(self, file_path: str, is_ivd: bool = False) -> dict:
        try:
            text = self.parse_document(file_path)
            device_info = self.extract_fields(text, is_ivd)
            
            if is_ivd and not device_info['intended_use']:
                raise ValueError("ä½“å¤–è¯Šæ–­è¯•å‰‚åˆ†ç±»å¿…é¡»æä¾›é¢„æœŸç”¨é€”")
            
            candidates = self.retrieve_candidates(device_info, is_ivd)
            prompt = self.generate_prompt(device_info, candidates, is_ivd)
            api_response = self.call_deepseek_api(prompt)
            
            json_match = re.search(r"```json\n(.+?)\n```", api_response, re.DOTALL)
            if not json_match:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONå“åº”")
            
            result = json.loads(json_match.group(1))
            result["is_ivd"] = is_ivd
            
            required_fields = ["classification", "confidence", "rationale"]
            if not all(field in result for field in required_fields):
                raise ValueError("APIè¿”å›ç¼ºå°‘å¿…è¦å­—æ®µ")
            
            return result
            
        except Exception as e:
            return {
                "error": str(e),
                "classification": "æœªçŸ¥",
                "confidence": 0,
                "rationale": "å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                "is_ivd": is_ivd
            }

def format_output(result: dict) -> str:
    if "error" in result:
        return f"âŒ å¤„ç†é”™è¯¯: {result['error']}"
    
    class_map = {
        "I": "â… ç±»ï¼ˆä½é£é™©ï¼‰",
        "II": "â…¡ç±»ï¼ˆä¸­é£é™©ï¼‰",
        "III": "â…¢ç±»ï¼ˆé«˜é£é™©ï¼‰",
        "unknown": "æœªçŸ¥ç±»åˆ«"
    }
    
    device_type = "ä½“å¤–è¯Šæ–­è¯•å‰‚" if result.get("is_ivd", False) else "åŒ»ç–—å™¨æ¢°"
    classification = class_map.get(result["classification"].upper(), class_map["unknown"])
    confidence = f"{result['confidence']*100:.1f}%" if 'confidence' in result else "æœªçŸ¥"
    matched_id = result.get("matched_id", "æ— ")
    rationale = result.get("rationale", "æ— è¯´æ˜")
    
    output = f"""## ğŸ¥ {device_type}åˆ†ç±»ç»“æœ

**ğŸ” åˆ†ç±»ç­‰çº§**: {classification}  
**ğŸ“Š ç½®ä¿¡åº¦**: {confidence}  
**ğŸ”— åŒ¹é…æ¡ç›®ID**: {matched_id}

### ğŸ“ åˆ†ç±»ä¾æ®
{rationale}
"""
    
    if result.get("missing_info"):
        output += f"\n\nâš ï¸ **éœ€è¦è¡¥å……çš„ä¿¡æ¯**: {', '.join(result['missing_info'])}"
    
    output += "\n\n---\n*æ³¨ï¼šæœ¬ç»“æœåŸºäºAIåˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚æ­£å¼åˆ†ç±»éœ€ä»¥ç›‘ç®¡éƒ¨é—¨è®¤å®šä¸ºå‡†ã€‚*"
    
    return output

def check_folder_structure(zip_file_path: str) -> str:
    if not os.path.isfile(zip_file_path) or not zip_file_path.lower().endswith('.zip'):
        return "âŒ ä¸Šä¼ æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ZIPå‹ç¼©æ–‡ä»¶"
    
    try:
        temp_dir = "temp_unzip"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir)
        
        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        
        missing_folders = []
        missing_files = []
        valid_folders = []
        
        top_level_folders = [d for d in os.listdir(temp_dir) if os.path.isdir(os.path.join(temp_dir, d))]
        
        if not top_level_folders:
            return "âŒ ZIPæ–‡ä»¶ä¸­ä¸åŒ…å«ä»»ä½•æ–‡ä»¶å¤¹"
        
        target_root = os.path.join(temp_dir, top_level_folders[0])
        
        for folder_name, required_files in REQUIRED_FOLDER_CONTENTS.items():
            folder_path = os.path.join(target_root, folder_name)
            
            if not os.path.exists(folder_path):
                missing_folders.append(folder_name)
                continue
            
            existing_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.docx')]
            missing = [f for f in required_files if f not in existing_files]
            
            if missing:
                missing_files.append(f"{folder_name}: {', '.join(missing)}")
            else:
                valid_folders.append(folder_name)
        
        result = "ğŸ“ **æ–‡ä»¶å¤¹å®Œæ•´æ€§æ£€æŸ¥ç»“æœ**\n\n"
        
        if missing_folders:
            result += f"âŒ **ç¼ºå¤±çš„æ–‡ä»¶å¤¹**: {', '.join(missing_folders)}\n\n"
        
        if missing_files:
            result += f"âŒ **ç¼ºå¤±çš„æ–‡ä»¶**:\n" + "\n".join([f"  - {item}" for item in missing_files]) + "\n\n"
        
        if valid_folders:
            result += f"âœ… **å®Œæ•´çš„æ–‡ä»¶å¤¹**: {', '.join(valid_folders)}\n\n"
        
        if not missing_folders and not missing_files:
            result += "ğŸ‰ **æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶å®Œæ•´ï¼Œç¬¦åˆè¦æ±‚ï¼**"
        else:
            result += "âš ï¸ **æ–‡ä»¶å¤¹æˆ–æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·è¡¥å……å®Œæ•´åå†æäº¤ã€‚**"
        
        shutil.rmtree(temp_dir)
        return result
    
    except Exception as e:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        return f"âŒ æ£€æŸ¥å¤±è´¥: {str(e)}"

classifier = MedicalDeviceClassifier()

with gr.Blocks(title="åŒ»ç–—å™¨æ¢°/IVDæ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""<h1 align="center">ğŸ¥ åŒ»ç–—å™¨æ¢°/ä½“å¤–è¯Šæ–­è¯•å‰‚åˆ†ç±»ç³»ç»Ÿ</h1>""")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
            file_input = gr.File(
                label="ä¸Šä¼ æ–‡æ¡£",
                file_types=[".docx"],
                type="filepath"
            )
            
            classify_type = gr.Radio(
                label="é€‰æ‹©åˆ†ç±»ç±»å‹",
                choices=["åŒ»ç–—å™¨æ¢°", "ä½“å¤–è¯Šæ–­è¯•å‰‚"],
                value="åŒ»ç–—å™¨æ¢°",
                interactive=True
            )
            
            submit_btn = gr.Button("å¼€å§‹åˆ†ç±»", variant="primary")
            
            gr.Markdown("### ğŸ“ æ–‡æ¡£è¦æ±‚")
            gr.Markdown("""
            - **åŒ»ç–—å™¨æ¢°**: éœ€åŒ…å«äº§å“æè¿°ã€é¢„æœŸç”¨é€”å’Œå“å
            - **ä½“å¤–è¯Šæ–­è¯•å‰‚**: å¿…é¡»åŒ…å«é¢„æœŸç”¨é€”ï¼ˆå“åå¯é€‰ï¼‰
            """)
            
            gr.Markdown("### ğŸ“ ä¸Šä¼ å®Œæ•´æ–‡ä»¶å¤¹ï¼ˆZIPå‹ç¼©ï¼‰")
            zip_input = gr.File(
                label="ä¸Šä¼ åŒ…å«å­æ–‡ä»¶å¤¹çš„ZIPå‹ç¼©æ–‡ä»¶",
                file_types=[".zip"]
            )
            zip_submit_btn = gr.Button("æ£€æŸ¥æ–‡ä»¶å¤¹å®Œæ•´æ€§")
            
            gr.Markdown("### ğŸ“ æ–‡ä»¶å¤¹åŠæ–‡ä»¶è¦æ±‚")
            gr.Markdown("""
            è¯·ç¡®ä¿ä¸Šä¼ çš„ZIPæ–‡ä»¶åŒ…å«ä»¥ä¸‹å­æ–‡ä»¶å¤¹åŠæ–‡ä»¶ï¼š
            - **1.ç›‘ç®¡ä¿¡æ¯**: 
              - 1.1 ç« èŠ‚ç›®å½•.docx
              - 1.2 ç”³è¯·è¡¨.docx
              - 1.3 æœ¯è¯­ã€ç¼©å†™è¯åˆ—è¡¨.docx
              - 1.4 äº§å“åˆ—è¡¨.docx
              - 1.5 å…³è”æ–‡ä»¶.docx
              - 1.6 ç”³æŠ¥å‰ä¸ç›‘ç®¡æœºæ„çš„è”ç³»æƒ…å†µå’Œæ²Ÿé€šè®°å½•.docx
              - 1.7 ç¬¦åˆæ€§å£°æ˜.docx
            - **2.ç»¼è¿°èµ„æ–™**: 
              - 2.1 ç« èŠ‚ç›®å½•.docx
              - 2.2 æ¦‚è¿°.docx
              - 2.3 äº§å“æè¿°.docx
              - 2.4 é€‚ç”¨èŒƒå›´å’Œç¦å¿Œè¯.docx
              - 2.5 ç”³æŠ¥äº§å“ä¸Šå¸‚å†å².docx
              - 2.6 å…¶ä»–éœ€è¯´æ˜çš„å†…å®¹.docx
            - **3.éä¸´åºŠèµ„æ–™**: 
              - 3.1 ç« èŠ‚ç›®å½•.docx
              - 3.2 äº§å“é£é™©ç®¡ç†èµ„æ–™.docx
              - 3.3 åŒ»ç–—å™¨æ¢°å®‰å…¨å’Œæ€§èƒ½åŸºæœ¬åŸåˆ™æ¸…å•.docx
              - 3.4 äº§å“æŠ€æœ¯è¦æ±‚åŠæ£€éªŒæŠ¥å‘Šç›¸å…³é™„ä»¶ä¸‹è½½.docx
              - 3.5 ç ”ç©¶èµ„æ–™.docx
              - 3.6 éä¸´åºŠæ–‡çŒ®.docx
              - 3.7 ç¨³å®šæ€§ç ”ç©¶.docx
              - 3.8 å…¶ä»–èµ„æ–™.docx
            - **4.ä¸´åºŠè¯„ä»·èµ„æ–™**: 
              - 4.1 ç« èŠ‚ç›®å½•.docx
              - 4.2 ä¸´åºŠè¯„ä»·èµ„æ–™.docx
              - 4.3 å…¶ä»–èµ„æ–™.docx
            - **5.äº§å“è¯´æ˜ä¹¦å’Œæ ‡ç­¾æ ·ç¨¿**: 
              - 5.1 ç« èŠ‚ç›®å½•.docx
              - 5.2 äº§å“è¯´æ˜ä¹¦.docx
              - 5.3 æ ‡ç­¾æ ·ç¨¿.docx
              - 5.4 å…¶ä»–èµ„æ–™.docx
            - **6.è´¨é‡ç®¡ç†ä½“ç³»æ–‡ä»¶**: 
              - 6.1 ç»¼è¿°.docx
              - 6.2 ç« èŠ‚ç›®å½•.docx
              - 6.3 ç”Ÿäº§åˆ¶é€ ä¿¡æ¯.docx
              - 6.4 è´¨é‡ç®¡ç†ä½“ç³»ç¨‹åº.docx
              - 6.5 ç®¡ç†èŒè´£ç¨‹åº.docx
              - 6.6 èµ„æºç®¡ç†ç¨‹åº.docx
              - 6.7 äº§å“å®ç°ç¨‹åº.docx
              - 6.8 è´¨é‡ç®¡ç†ä½“ç³»çš„æµ‹é‡ã€åˆ†æå’Œæ”¹è¿›ç¨‹åº.docx
              - 6.9 å…¶ä»–è´¨é‡ä½“ç³»ç¨‹åºä¿¡æ¯.docx
              - 6.10 è´¨é‡ç®¡ç†ä½“ç³»æ ¸æŸ¥æ–‡ä»¶.docx
            """)
        
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ“Š åˆ†ç±»ç»“æœ")
            output = gr.Markdown(
                label="åˆ†æç»“æœ",
                value="ç­‰å¾…åˆ†æ...",
                show_copy_button=True
            )
            
            folder_output = gr.Markdown(
                label="æ–‡ä»¶å¤¹æ£€æŸ¥ç»“æœ",
                value="ç­‰å¾…æ£€æŸ¥...",
                show_copy_button=True
            )
    
    submit_btn.click(
        fn=lambda f, t: format_output(classifier.process_document(f, t=="ä½“å¤–è¯Šæ–­è¯•å‰‚")),
        inputs=[file_input, classify_type],
        outputs=output
    )
    
    zip_submit_btn.click(
        fn=check_folder_structure,
        inputs=zip_input,
        outputs=folder_output
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7863,
        show_error=True,
        share = True
    )