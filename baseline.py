# ä¸å«ä½“å¤–è¯•å‰‚æ£€æµ‹
import gradio as gr
import pandas as pd
import json
import re
import os
import docx
import requests
import numpy as np
from typing import List, Dict
from sentence_transformers import SentenceTransformer, util
import torch
from pathlib import Path
import zipfile
import shutil

# ======================
# é…ç½®éƒ¨åˆ†
# ======================
# DeepSeek APIé…ç½®
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEEPSEEK_API_KEY = "sk-883d825876464ab6966616a3ae887953"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…API Key
KNOWLEDGE_BASE_PATH = "/home/dockeruser/lmy/äºŒçº§åŒ»ç–—å™¨æ¢°/uft82.csv"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„

# åµŒå…¥æ¨¡å‹é…ç½® - æœ¬åœ°æ¨¡å‹è·¯å¾„
LOCAL_MODEL_PATH = "/home/dockeruser/lmy/Model/model1"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
EMBEDDING_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  # è‡ªåŠ¨æ£€æµ‹GPU

# æ–‡ä»¶å¤¹ç»“æ„å’Œæ–‡ä»¶è¦æ±‚
REQUIRED_FOLDER_CONTENTS = {
    "1.ç›‘ç®¡ä¿¡æ¯": [
        "1.1ç« èŠ‚ç›®å½•.docx",
        "1.2ç”³è¯·è¡¨.docx",
        "1.3æœ¯è¯­ã€ç¼©å†™è¯åˆ—è¡¨.docx",
        "1.4äº§å“åˆ—è¡¨.docx",
        "1.5å…³è”æ–‡ä»¶.docx",
        "1.6ç”³æŠ¥å‰ä¸ç›‘ç®¡æœºæ„çš„è”ç³»æƒ…å†µå’Œæ²Ÿé€šè®°å½•.docx",
        "1.7ç¬¦åˆæ€§å£°æ˜.docx"
    ],
    "2.ç»¼è¿°èµ„æ–™": [
        "2.1ç« èŠ‚ç›®å½•.docx",
        "2.2æ¦‚è¿°.docx",
        "2.3äº§å“æè¿°.docx",
        "2.4é€‚ç”¨èŒƒå›´å’Œç¦å¿Œè¯.docx",
        "2.5ç”³æŠ¥äº§å“ä¸Šå¸‚å†å².docx",
        "2.6å…¶ä»–éœ€è¯´æ˜çš„å†…å®¹.docx"
    ],
    "3.éä¸´åºŠèµ„æ–™": [
        "3.1ç« èŠ‚ç›®å½•.docx",
        "3.2äº§å“é£é™©ç®¡ç†èµ„æ–™.docx",
        "3.3åŒ»ç–—å™¨æ¢°å®‰å…¨å’Œæ€§èƒ½åŸºæœ¬åŸåˆ™æ¸…å•.docx",
        "3.4äº§å“æŠ€æœ¯è¦æ±‚åŠæ£€éªŒæŠ¥å‘Šç›¸å…³é™„ä»¶ä¸‹è½½.docx",
        "3.5ç ”ç©¶èµ„æ–™.docx",
        "3.6éä¸´åºŠæ–‡çŒ®.docx",
        "3.7ç¨³å®šæ€§ç ”ç©¶.docx",
        "3.8å…¶ä»–èµ„æ–™.docx"
    ],
    "4.ä¸´åºŠè¯„ä»·èµ„æ–™": [
        "4.1ç« èŠ‚ç›®å½•.docx",
        "4.2ä¸´åºŠè¯„ä»·èµ„æ–™.docx",
        "4.3å…¶ä»–èµ„æ–™.docx"
    ],
    "5.äº§å“è¯´æ˜ä¹¦å’Œæ ‡ç­¾æ ·ç¨¿": [
        "5.1ç« èŠ‚ç›®å½•.docx",
        "5.2äº§å“è¯´æ˜ä¹¦.docx",
        "5.3æ ‡ç­¾æ ·ç¨¿.docx",
        "5.4å…¶ä»–èµ„æ–™.docx"
    ],
    "6.è´¨é‡ç®¡ç†ä½“ç³»æ–‡ä»¶": [
        "6.1ç»¼è¿°.docx",
        "6.2ç« èŠ‚ç›®å½•.docx",
        "6.3ç”Ÿäº§åˆ¶é€ ä¿¡æ¯.docx",
        "6.4è´¨é‡ç®¡ç†ä½“ç³»ç¨‹åº.docx",
        "6.5ç®¡ç†èŒè´£ç¨‹åº.docx",
        "6.6èµ„æºç®¡ç†ç¨‹åº.docx",
        "6.7äº§å“å®ç°ç¨‹åº.docx",
        "6.8è´¨é‡ç®¡ç†ä½“ç³»çš„æµ‹é‡ã€åˆ†æå’Œæ”¹è¿›ç¨‹åº.docx",
        "6.9å…¶ä»–è´¨é‡ä½“ç³»ç¨‹åºä¿¡æ¯.docx",
        "6.10è´¨é‡ç®¡ç†ä½“ç³»æ ¸æŸ¥æ–‡ä»¶.docx"
    ]
}

# ======================
# ç³»ç»Ÿåˆå§‹åŒ–
# ======================
class MedicalDeviceClassifier:
    def __init__(self):
        self.knowledge_base = None
        self.embedding_model = None
        self.embeddings = None
        self.initialize_knowledge_base()

    def initialize_knowledge_base(self):
        """åŠ è½½å¹¶åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ˆä½¿ç”¨æœ¬åœ°å‘é‡åµŒå…¥æ¨¡å‹ï¼‰"""
        try:
            # è¯»å–çŸ¥è¯†åº“CSVæ–‡ä»¶
            if not os.path.exists(KNOWLEDGE_BASE_PATH):
                raise FileNotFoundError(f"çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {KNOWLEDGE_BASE_PATH}")
            
            df = pd.read_csv(KNOWLEDGE_BASE_PATH)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_columns = ['row_id', 'desc', 'intended_use', 'name', 'grade']
            if not all(col in df.columns for col in required_columns):
                raise ValueError("CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—")
            
            self.knowledge_base = df.to_dict('records')
            
            # ä»æœ¬åœ°è·¯å¾„åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹({LOCAL_MODEL_PATH})ï¼Œè®¾å¤‡: {EMBEDDING_DEVICE}...")
            if not os.path.exists(LOCAL_MODEL_PATH):
                raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {LOCAL_MODEL_PATH}")
                
            self.embedding_model = SentenceTransformer(
                model_name_or_path=LOCAL_MODEL_PATH,
                device=EMBEDDING_DEVICE
            )
            
            # ä¸ºçŸ¥è¯†åº“ç”ŸæˆåµŒå…¥å‘é‡
            print("æ­£åœ¨ç”ŸæˆçŸ¥è¯†åº“åµŒå…¥å‘é‡...")
            corpus = [
                f"{row['name']}ã€‚{row['desc']}ã€‚ç”¨äº{row['intended_use']}" 
                for row in self.knowledge_base
            ]
            self.embeddings = self.embedding_model.encode(
                corpus, 
                show_progress_bar=True,
                convert_to_tensor=True,
                normalize_embeddings=True,
                batch_size=32  # æ ¹æ®å†…å­˜è°ƒæ•´æ‰¹å¤§å°
            )
            
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸï¼Œå…±åŠ è½½{len(self.knowledge_base)}æ¡è®°å½•")
        except Exception as e:
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def call_deepseek_api(self, prompt: str) -> str:
        """è°ƒç”¨DeepSeek APIè¿›è¡Œæ¨ç†"""
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—å™¨æ¢°åˆ†ç±»ä¸“å®¶ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
1. å¿…é¡»æ˜ç¡®ç»™å‡ºåŒ»ç–—å™¨æ¢°åˆ†ç±»ç­‰çº§ï¼ˆIç±»/IIç±»/IIIç±»ï¼‰
2. å¿…é¡»åŸºäºæä¾›çš„çŸ¥è¯†åº“æ¡ç›®è¿›è¡Œåˆ¤æ–­
3. æœ€ç»ˆå›ç­”å¿…é¡»åŒ…å«ä»¥ä¸‹JSONç»“æ„ï¼š
```json
{
  "classification": "I/II/III",
  "confidence": 0.0-1.0,
  "rationale": "åˆ†ç±»ä¾æ®è¯´æ˜",
  "matched_id": åŒ¹é…çš„æ¡ç›®ID,
  "missing_info": ["éœ€è¦è¡¥å……çš„å­—æ®µ"]
}
```"""
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.1,
            "max_tokens": 800
        }
        
        try:
            response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
        except KeyError:
            raise RuntimeError("APIè¿”å›æ ¼å¼å¼‚å¸¸")

    def parse_document(self, file_path: str) -> str:
        """ä¼˜åŒ–ç‰ˆDOCXæ–‡æ¡£è§£æ
        ä¸“æ³¨äºæå–æ–‡æœ¬å†…å®¹ï¼Œåˆå¹¶è¢«é”™è¯¯åˆ†æ®µçš„å¥å­
        """
        try:
            doc = docx.Document(file_path)
            full_text = []
            current_line = ""
            
            for para in doc.paragraphs:
                text = para.text.strip()
                
                if not text:
                    if current_line:  # é‡åˆ°ç©ºè¡Œæ—¶æäº¤å½“å‰è¡Œ
                        full_text.append(current_line)
                        current_line = ""
                    continue
                    
                # åˆ¤æ–­æ˜¯å¦æ˜¯å¥å­ç»“å°¾ï¼ˆä¸­æ–‡æ ‡ç‚¹ï¼‰
                if text.endswith(('ã€‚', 'ï¼›', 'ï¼', 'ï¼Ÿ', 'ï¼‰', 'ã€', '.', ';', '!', '?')):
                    if current_line:
                        full_text.append(current_line + text)
                        current_line = ""
                    else:
                        full_text.append(text)
                else:
                    current_line += text
            
            # æ·»åŠ æœ€åæœªå®Œæˆçš„è¡Œ
            if current_line:
                full_text.append(current_line)
                
            return "\n".join(full_text)
        except Exception as e:
            raise ValueError(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")

    def extract_fields(self, text: str) -> dict:
        """ç²¾å‡†å­—æ®µæå–
        ä¸“æ³¨äºæå–äº§å“æè¿°ã€é¢„æœŸç”¨é€”å’Œå“åä¸‰ä¸ªå…³é”®å­—æ®µ
        """
        # é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼
        text = re.sub(r'[:ï¼š]\s*', ': ', text)  # ç»Ÿä¸€åˆ†éš”ç¬¦æ ¼å¼
        
        # å®šä¹‰å¤šçº§åŒ¹é…æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        patterns = {
            "desc": [
                r"äº§å“æè¿°[:ï¼š]\s*(.+?)(?=\n|$)",  # å¸¦æ ‡ç­¾æ ¼å¼
                r"ã€äº§å“æè¿°ã€‘(.+?)(?=ã€|$)",      # æ–¹æ‹¬å·æ ¼å¼
                r"æè¿°[:ï¼š]\s*(.+?)(?=\n|$)",     # ç®€ç•¥æ ‡ç­¾
                r"(?<=äº§å“æ¦‚è¿°[:ï¼š]).+?(?=\n|$)"  # åå‘æ–­è¨€
            ],
            "intended_use": [
                r"é¢„æœŸç”¨é€”[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ã€é¢„æœŸç”¨é€”ã€‘(.+?)(?=ã€|$)",
                r"ç”¨é€”[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ç”¨äº(.+?)(?=\n|$)",             # æ— æ ‡ç­¾æ ¼å¼
                r"é€‚ç”¨èŒƒå›´[:ï¼š]\s*(.+?)(?=\n|$)"
            ],
            "name": [
                r"å“å[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ã€äº§å“åç§°ã€‘(.+?)(?=ã€|$)",
                r"åç§°[:ï¼š]\s*(.+?)(?=\n|$)",
                r"äº§å“å[:ï¼š]\s*(.+?)(?=\n|$)",
                r"æ³¨å†Œåç§°[:ï¼š]\s*(.+?)(?=\n|$)"
            ]
        }
        
        result = {}
        for field, field_patterns in patterns.items():
            # å°è¯•æ‰€æœ‰æ¨¡å¼ç›´åˆ°åŒ¹é…æˆåŠŸ
            for pattern in field_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    extracted = match.group(1).strip()
                    # ç®€å•æ¸…ç†æå–ç»“æœ
                    extracted = re.sub(r'^[:ï¼š\s]+|[:ï¼š\s]+$', '', extracted)
                    result[field] = extracted
                    break
            else:
                result[field] = ""
        
        # æ™ºèƒ½å›é€€ï¼šå¦‚æœæ ‡å‡†æ¨¡å¼æ²¡æ‰¾åˆ°ï¼Œå°è¯•é¡ºåºæå–
        if not all(result.values()):
            lines = [line for line in text.split('\n') if line.strip() and len(line) > 10]
            if len(lines) >= 3:
                # æ™ºèƒ½åˆ†é…ï¼šæœ€é•¿çš„é€šå¸¸æ˜¯æè¿°ï¼ŒåŒ…å«"ç”¨äº"çš„æ˜¯ç”¨é€”ï¼Œæœ€çŸ­çš„æ˜¯åç§°
                lines_sorted = sorted(lines, key=len)
                if not result["desc"]:
                    result["desc"] = lines_sorted[-1]  # æœ€é•¿çš„
                if not result["intended_use"] and "ç”¨äº" in text:
                    for line in lines:
                        if "ç”¨äº" in line:
                            result["intended_use"] = line
                            break
                if not result["name"]:
                    result["name"] = lines_sorted[0]  # æœ€çŸ­çš„
        
        # æœ€ç»ˆéªŒè¯å’Œæ¸…ç†
        result = {k: v.strip() for k, v in result.items()}
        
        # éªŒè¯å­—æ®µå®Œæ•´æ€§
        missing = [k for k, v in result.items() if not v]
        if missing:
            raise ValueError(f"æ–‡æ¡£ä¸­ç¼ºå°‘ä»¥ä¸‹å¿…è¦å­—æ®µ: {', '.join(missing)}")
        
        return result

    def retrieve_candidates(self, query: dict, top_k: int = 3) -> List[Dict]:
        """è¯­ä¹‰æ£€ç´¢æœ€ç›¸ä¼¼çš„å€™é€‰æ¡ç›®"""
        if not self.knowledge_base or self.embeddings is None:
            raise RuntimeError("çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_text = f"{query['name']}ã€‚{query['desc']}ã€‚ç”¨äº{query['intended_use']}"
        query_embedding = self.embedding_model.encode(
            query_text, 
            convert_to_tensor=True,
            normalize_embeddings=True
        )
        
        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        cos_scores = util.pytorch_cos_sim(query_embedding, self.embeddings)[0]
        top_results = torch.topk(cos_scores, k=top_k)
        
        # ç»„åˆç»“æœ
        results = []
        for score, idx in zip(top_results[0], top_results[1]):
            row = self.knowledge_base[idx]
            results.append({
                "row_id": row["row_id"],
                "desc": row["desc"],
                "intended_use": row["intended_use"],
                "name": row["name"],
                "grade": row["grade"],
                "similarity": float(score)  # è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
            })
        
        return results

    def generate_prompt(self, device_info: dict, candidates: list) -> str:
        """ç”Ÿæˆç»™LLMçš„æç¤ºè¯"""
        candidate_str = "\n".join(
            f"ã€æ¡ç›®#{c['row_id']}ã€‘\n"
            f"æè¿°: {c['desc']}\n"
            f"ç”¨é€”: {c['intended_use']}\n"
            f"åç§°: {c['name']}\n"
            f"ç±»åˆ«: {c['grade']}ç±»\n"
            f"è¯­ä¹‰ç›¸ä¼¼åº¦: {c['similarity']:.2f}\n"
            for c in candidates
        )
        
        return f"""è¯·å¯¹ä»¥ä¸‹åŒ»ç–—å™¨æ¢°è¿›è¡Œåˆ†ç±»åˆ†æï¼š

===== å¾…åˆ†ç±»è®¾å¤‡ =====
æè¿°: {device_info['desc']}
ç”¨é€”: {device_info['intended_use']}
åç§°: {device_info['name']}

===== çŸ¥è¯†åº“å€™é€‰æ¡ç›® =====
{candidate_str}

è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¿”å›JSONæ ¼å¼ç»“æœï¼Œå¿…é¡»åŒ…å«ï¼š
1. æ˜ç¡®çš„åˆ†ç±»ç­‰çº§ï¼ˆI/II/IIIï¼‰
2. ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
3. è¯¦ç»†çš„åˆ†ç±»ä¾æ®
4. åŒ¹é…çš„æ¡ç›®IDï¼ˆå¦‚æ— å¯å†™-1ï¼‰
5. éœ€è¦è¡¥å……çš„ä¿¡æ¯ï¼ˆå¦‚æ— å¯å†™ç©ºåˆ—è¡¨ï¼‰"""

    def process_document(self, file_path: str) -> dict:
        """å¤„ç†æ–‡æ¡£çš„ä¸»æµç¨‹"""
        try:
            # 1. è§£ææ–‡æ¡£
            text = self.parse_document(file_path)
            device_info = self.extract_fields(text)
            
            # 2. è¯­ä¹‰æ£€ç´¢å€™é€‰
            candidates = self.retrieve_candidates(device_info)
            
            # 3. ç”ŸæˆPromptå¹¶è°ƒç”¨API
            prompt = self.generate_prompt(device_info, candidates)
            api_response = self.call_deepseek_api(prompt)
            
            # 4. æå–JSONç»“æœ
            json_match = re.search(r"```json\n(.+?)\n```", api_response, re.DOTALL)
            if not json_match:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONå“åº”")
            
            result = json.loads(json_match.group(1))
            
            # éªŒè¯ç»“æœæ ¼å¼
            required_fields = ["classification", "confidence", "rationale"]
            if not all(field in result for field in required_fields):
                raise ValueError("APIè¿”å›ç¼ºå°‘å¿…è¦å­—æ®µ")
            
            return result
            
        except Exception as e:
            return {
                "error": str(e),
                "classification": "æœªçŸ¥",
                "confidence": 0,
                "rationale": "å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"
            }

def format_output(result: dict) -> str:
    """æ ¼å¼åŒ–è¾“å‡ºç»“æœ"""
    if "error" in result:
        return f"âŒ å¤„ç†é”™è¯¯: {result['error']}"
    
    # åˆ†ç±»ç­‰çº§æ˜ å°„
    class_map = {
        "I": "â… ç±»ï¼ˆä½é£é™©ï¼‰",
        "II": "â…¡ç±»ï¼ˆä¸­é£é™©ï¼‰",
        "III": "â…¢ç±»ï¼ˆé«˜é£é™©ï¼‰",
        "unknown": "æœªçŸ¥ç±»åˆ«"
    }
    
    classification = class_map.get(result["classification"].upper(), class_map["unknown"])
    confidence = f"{result['confidence']*100:.1f}%" if 'confidence' in result else "æœªçŸ¥"
    matched_id = result.get("matched_id", "æ— ")
    rationale = result.get("rationale", "æ— è¯´æ˜")
    
    # æ„å»ºMarkdownè¾“å‡º
    output = f"""## ğŸ¥ åŒ»ç–—å™¨æ¢°åˆ†ç±»ç»“æœ

**ğŸ” åˆ†ç±»ç­‰çº§**: {classification}  
**ğŸ“Š ç½®ä¿¡åº¦**: {confidence}  
**ğŸ”— åŒ¹é…æ¡ç›®ID**: {matched_id}

### ğŸ“ åˆ†ç±»ä¾æ®
{rationale}
"""
    
    # æ·»åŠ è¡¥å……ä¿¡æ¯æç¤º
    if result.get("missing_info"):
        output += f"\n\nâš ï¸ **éœ€è¦è¡¥å……çš„ä¿¡æ¯**: {', '.join(result['missing_info'])}"
    
    # æ·»åŠ å…è´£å£°æ˜
    output += "\n\n---\n*æ³¨ï¼šæœ¬ç»“æœåŸºäºAIåˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚æ­£å¼åˆ†ç±»éœ€ä»¥ç›‘ç®¡éƒ¨é—¨è®¤å®šä¸ºå‡†ã€‚*"
    
    return output

def check_folder_structure(zip_file_path: str) -> str:
    """æ£€æŸ¥ä¸Šä¼ ZIPæ–‡ä»¶ä¸­çš„æ–‡ä»¶å¤¹ç»“æ„å’Œæ–‡ä»¶å®Œæ•´æ€§"""
    if not os.path.isfile(zip_file_path) or not zip_file_path.lower().endswith('.zip'):
        return "âŒ ä¸Šä¼ æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ZIPå‹ç¼©æ–‡ä»¶"
    
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•è§£å‹ZIPæ–‡ä»¶
        temp_dir = "temp_unzip"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir)
        
        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        
        # æ£€æŸ¥è§£å‹åçš„æ–‡ä»¶å¤¹ç»“æ„å’Œæ–‡ä»¶å®Œæ•´æ€§
        missing_folders = []
        missing_files = []
        valid_folders = []
        
        # å‡è®¾ZIPæ–‡ä»¶ä¸­åªæœ‰ä¸€ä¸ªé¡¶çº§æ–‡ä»¶å¤¹
        top_level_folders = [d for d in os.listdir(temp_dir) if os.path.isdir(os.path.join(temp_dir, d))]
        
        if not top_level_folders:
            return "âŒ ZIPæ–‡ä»¶ä¸­ä¸åŒ…å«ä»»ä½•æ–‡ä»¶å¤¹"
        
        # å‡è®¾ç¬¬ä¸€ä¸ªé¡¶çº§æ–‡ä»¶å¤¹æ˜¯ç›®æ ‡æ–‡ä»¶å¤¹
        target_root = os.path.join(temp_dir, top_level_folders[0])
        
        for folder_name, required_files in REQUIRED_FOLDER_CONTENTS.items():
            # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
            folder_path = os.path.join(target_root, folder_name)
            
            if not os.path.exists(folder_path):
                missing_folders.append(folder_name)
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶
            existing_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.docx')]
            missing = [f for f in required_files if f not in existing_files]
            
            if missing:
                missing_files.append(f"{folder_name}: {', '.join(missing)}")
            else:
                valid_folders.append(folder_name)
        
        # æ„å»ºç»“æœæŠ¥å‘Š
        result = "ğŸ“ **æ–‡ä»¶å¤¹å®Œæ•´æ€§æ£€æŸ¥ç»“æœ**\n\n"
        
        if missing_folders:
            result += f"âŒ **ç¼ºå¤±çš„æ–‡ä»¶å¤¹**: {', '.join(missing_folders)}\n\n"
        
        if missing_files:
            result += f"âŒ **ç¼ºå¤±çš„æ–‡ä»¶**:\n" + "\n".join([f"  - {item}" for item in missing_files]) + "\n\n"
        
        if valid_folders:
            result += f"âœ… **å®Œæ•´çš„æ–‡ä»¶å¤¹**: {', '.join(valid_folders)}\n\n"
        
        if not missing_folders and not missing_files:
            result += "ğŸ‰ **æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶å®Œæ•´ï¼Œç¬¦åˆè¦æ±‚ï¼**"
        else:
            result += "âš ï¸ **æ–‡ä»¶å¤¹æˆ–æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·è¡¥å……å®Œæ•´åå†æäº¤ã€‚**"
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir)
        
        return result
    
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        return f"âŒ æ£€æŸ¥å¤±è´¥: {str(e)}"

# åˆå§‹åŒ–åˆ†ç±»å™¨
classifier = MedicalDeviceClassifier()

with gr.Blocks(title="åŒ»ç–—å™¨æ¢°æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""<h1 align="center">ğŸ¥ åŒ»ç–—å™¨æ¢°åˆ†ç±»ç³»ç»Ÿï¼ˆæœ¬åœ°æ¨¡å‹ç‰ˆï¼‰</h1>""")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
            file_input = gr.File(
                label="ä¸Šä¼ åŒ»ç–—å™¨æ¢°æ–‡æ¡£",
                file_types=[".docx"],
                type="filepath"
            )
            submit_btn = gr.Button("å¼€å§‹åˆ†ç±»", variant="primary")
            
            gr.Markdown("### ğŸ“ æ–‡æ¡£æ ¼å¼è¦æ±‚")
            gr.Markdown("""
            è¯·ç¡®ä¿æ–‡æ¡£åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - **äº§å“æè¿°**: å™¨æ¢°çš„åŠŸèƒ½å’Œç‰¹å¾
            - **é¢„æœŸç”¨é€”**: ä¸´åºŠç”¨é€”å’Œä½¿ç”¨æ–¹æ³•
            - **å“å**: äº§å“æ³¨å†Œåç§°
            
            ç¤ºä¾‹æ ¼å¼ï¼š
            `````
            äº§å“æè¿°: é«˜é¢‘å°„é¢‘æ‰‹æœ¯ç³»ç»Ÿ
            é¢„æœŸç”¨é€”: ç”¨äºå¤–ç§‘æ‰‹æœ¯ä¸­çš„ç»„ç»‡åˆ‡å‰²å’Œå‡è¡€
            å“å: é«˜é¢‘ç”µåˆ€
            ````
            """)
            
            # ZIPæ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
            gr.Markdown("### ğŸ“ ä¸Šä¼ å®Œæ•´æ–‡ä»¶å¤¹ï¼ˆZIPå‹ç¼©ï¼‰")
            zip_input = gr.File(
                label="ä¸Šä¼ åŒ…å«å­æ–‡ä»¶å¤¹çš„ZIPå‹ç¼©æ–‡ä»¶",
                file_types=[".zip"]
            )
            zip_submit_btn = gr.Button("æ£€æŸ¥æ–‡ä»¶å¤¹å®Œæ•´æ€§")
            
            gr.Markdown("### ğŸ“ æ–‡ä»¶å¤¹åŠæ–‡ä»¶è¦æ±‚")
            gr.Markdown("""
            è¯·ç¡®ä¿ä¸Šä¼ çš„ZIPæ–‡ä»¶åŒ…å«ä»¥ä¸‹å­æ–‡ä»¶å¤¹åŠæ–‡ä»¶(åºå·ä¸å†…å®¹ä¹‹é—´æ²¡æœ‰ç©ºæ ¼)ï¼š
            - **1.ç›‘ç®¡ä¿¡æ¯**: 
              - 1.1 ç« èŠ‚ç›®å½•.docx
              - 1.2 ç”³è¯·è¡¨.docx
              - 1.3 æœ¯è¯­ã€ç¼©å†™è¯åˆ—è¡¨.docx
              - 1.4 äº§å“åˆ—è¡¨.docx
              - 1.5 å…³è”æ–‡ä»¶.docx
              - 1.6 ç”³æŠ¥å‰ä¸ç›‘ç®¡æœºæ„çš„è”ç³»æƒ…å†µå’Œæ²Ÿé€šè®°å½•.docx
              - 1.7 ç¬¦åˆæ€§å£°æ˜.docx
            - **2.ç»¼è¿°èµ„æ–™**: 
              - 2.1 ç« èŠ‚ç›®å½•.docx
              - 2.2 æ¦‚è¿°.docx
              - 2.3 äº§å“æè¿°.docx
              - 2.4 é€‚ç”¨èŒƒå›´å’Œç¦å¿Œè¯.docx
              - 2.5 ç”³æŠ¥äº§å“ä¸Šå¸‚å†å².docx
              - 2.6 å…¶ä»–éœ€è¯´æ˜çš„å†…å®¹.docx
            - **3.éä¸´åºŠèµ„æ–™**: 
              - 3.1 ç« èŠ‚ç›®å½•.docx
              - 3.2 äº§å“é£é™©ç®¡ç†èµ„æ–™.docx
              - 3.3 åŒ»ç–—å™¨æ¢°å®‰å…¨å’Œæ€§èƒ½åŸºæœ¬åŸåˆ™æ¸…å•.docx
              - 3.4 äº§å“æŠ€æœ¯è¦æ±‚åŠæ£€éªŒæŠ¥å‘Šç›¸å…³é™„ä»¶ä¸‹è½½.docx
              - 3.5 ç ”ç©¶èµ„æ–™.docx
              - 3.6 éä¸´åºŠæ–‡çŒ®.docx
              - 3.7 ç¨³å®šæ€§ç ”ç©¶.docx
              - 3.8 å…¶ä»–èµ„æ–™.docx
            - **4.ä¸´åºŠè¯„ä»·èµ„æ–™**: 
              - 4.1 ç« èŠ‚ç›®å½•.docx
              - 4.2 ä¸´åºŠè¯„ä»·èµ„æ–™.docx
              - 4.3 å…¶ä»–èµ„æ–™.docx
            - **5.äº§å“è¯´æ˜ä¹¦å’Œæ ‡ç­¾æ ·ç¨¿**: 
              - 5.1 ç« èŠ‚ç›®å½•.docx
              - 5.2 äº§å“è¯´æ˜ä¹¦.docx
              - 5.3 æ ‡ç­¾æ ·ç¨¿.docx
              - 5.4 å…¶ä»–èµ„æ–™.docx
            - **6.è´¨é‡ç®¡ç†ä½“ç³»æ–‡ä»¶**: 
              - 6.1 ç»¼è¿°.docx
              - 6.2 ç« èŠ‚ç›®å½•.docx
              - 6.3 ç”Ÿäº§åˆ¶é€ ä¿¡æ¯.docx
              - 6.4 è´¨é‡ç®¡ç†ä½“ç³»ç¨‹åº.docx
              - 6.5 ç®¡ç†èŒè´£ç¨‹åº.docx
              - 6.6 èµ„æºç®¡ç†ç¨‹åº.docx
              - 6.7 äº§å“å®ç°ç¨‹åº.docx
              - 6.8 è´¨é‡ç®¡ç†ä½“ç³»çš„æµ‹é‡ã€åˆ†æå’Œæ”¹è¿›ç¨‹åº.docx
              - 6.9 å…¶ä»–è´¨é‡ä½“ç³»ç¨‹åºä¿¡æ¯.docx
              - 6.10 è´¨é‡ç®¡ç†ä½“ç³»æ ¸æŸ¥æ–‡ä»¶.docx
            """)
        
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ“Š åˆ†ç±»ç»“æœ")
            output = gr.Markdown(
                label="åˆ†æç»“æœ",
                value="ç­‰å¾…åˆ†æ...",
                show_copy_button=True
            )
            
            # æ–‡ä»¶å¤¹æ£€æŸ¥ç»“æœå±•ç¤º
            folder_output = gr.Markdown(
                label="æ–‡ä»¶å¤¹æ£€æŸ¥ç»“æœ",
                value="ç­‰å¾…æ£€æŸ¥...",
                show_copy_button=True
            )
    
    # å¤„ç†é€»è¾‘
    submit_btn.click(
        fn=lambda f: format_output(classifier.process_document(f)),
        inputs=file_input,
        outputs=output
    )
    
    zip_submit_btn.click(
        fn=check_folder_structure,
        inputs=zip_input,
        outputs=folder_output
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7862,
        show_error=True
    )                                            