import gradio as gr
import pandas as pd
import json
import re
import os
import docx
import requests
import numpy as np
from typing import List, Dict
from sentence_transformers import SentenceTransformer, util
import torch

# ======================
# é…ç½®éƒ¨åˆ†
# ======================
# DeepSeek APIé…ç½®
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEEPSEEK_API_KEY = "sk-883d825876464ab6966616a3ae887953"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…API Key
KNOWLEDGE_BASE_PATH = "/home/dockeruser/lmy/äºŒçº§åŒ»ç–—å™¨æ¢°/uft82.csv"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„

# åµŒå…¥æ¨¡å‹é…ç½® - æœ¬åœ°æ¨¡å‹è·¯å¾„
LOCAL_MODEL_PATH = "/home/dockeruser/lmy/Model/model"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
EMBEDDING_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  # è‡ªåŠ¨æ£€æµ‹GPU

# ======================
# ç³»ç»Ÿåˆå§‹åŒ–
# ======================
class MedicalDeviceClassifier:
    def __init__(self):
        self.knowledge_base = None
        self.embedding_model = None
        self.embeddings = None
        self.initialize_knowledge_base()

    def initialize_knowledge_base(self):
        """åŠ è½½å¹¶åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ˆä½¿ç”¨æœ¬åœ°å‘é‡åµŒå…¥æ¨¡å‹ï¼‰"""
        try:
            # è¯»å–çŸ¥è¯†åº“CSVæ–‡ä»¶
            if not os.path.exists(KNOWLEDGE_BASE_PATH):
                raise FileNotFoundError(f"çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {KNOWLEDGE_BASE_PATH}")
            
            df = pd.read_csv(KNOWLEDGE_BASE_PATH)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_columns = ['row_id', 'desc', 'intended_use', 'name', 'grade']
            if not all(col in df.columns for col in required_columns):
                raise ValueError("CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—")
            
            self.knowledge_base = df.to_dict('records')
            
            # ä»æœ¬åœ°è·¯å¾„åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹({LOCAL_MODEL_PATH})ï¼Œè®¾å¤‡: {EMBEDDING_DEVICE}...")
            if not os.path.exists(LOCAL_MODEL_PATH):
                raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {LOCAL_MODEL_PATH}")
                
            self.embedding_model = SentenceTransformer(
                model_name_or_path=LOCAL_MODEL_PATH,
                device=EMBEDDING_DEVICE
            )
            
            # ä¸ºçŸ¥è¯†åº“ç”ŸæˆåµŒå…¥å‘é‡
            print("æ­£åœ¨ç”ŸæˆçŸ¥è¯†åº“åµŒå…¥å‘é‡...")
            corpus = [
                f"{row['name']}ã€‚{row['desc']}ã€‚ç”¨äº{row['intended_use']}" 
                for row in self.knowledge_base
            ]
            self.embeddings = self.embedding_model.encode(
                corpus, 
                show_progress_bar=True,
                convert_to_tensor=True,
                normalize_embeddings=True,
                batch_size=32  # æ ¹æ®å†…å­˜è°ƒæ•´æ‰¹å¤§å°
            )
            
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸï¼Œå…±åŠ è½½{len(self.knowledge_base)}æ¡è®°å½•")
        except Exception as e:
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def call_deepseek_api(self, prompt: str) -> str:
        """è°ƒç”¨DeepSeek APIè¿›è¡Œæ¨ç†"""
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—å™¨æ¢°åˆ†ç±»ä¸“å®¶ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
1. å¿…é¡»æ˜ç¡®ç»™å‡ºåŒ»ç–—å™¨æ¢°åˆ†ç±»ç­‰çº§ï¼ˆIç±»/IIç±»/IIIç±»ï¼‰
2. å¿…é¡»åŸºäºæä¾›çš„çŸ¥è¯†åº“æ¡ç›®è¿›è¡Œåˆ¤æ–­
3. æœ€ç»ˆå›ç­”å¿…é¡»åŒ…å«ä»¥ä¸‹JSONç»“æ„ï¼š
```json
{
  "classification": "I/II/III",
  "confidence": 0.0-1.0,
  "rationale": "åˆ†ç±»ä¾æ®è¯´æ˜",
  "matched_id": åŒ¹é…çš„æ¡ç›®ID,
  "missing_info": ["éœ€è¦è¡¥å……çš„å­—æ®µ"]
}
```"""
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.1,
            "max_tokens": 800
        }
        
        try:
            response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
        except KeyError:
            raise RuntimeError("APIè¿”å›æ ¼å¼å¼‚å¸¸")

    def parse_document(self, file_path: str) -> str:
        """ä¼˜åŒ–ç‰ˆDOCXæ–‡æ¡£è§£æ
        ä¸“æ³¨äºæå–æ–‡æœ¬å†…å®¹ï¼Œåˆå¹¶è¢«é”™è¯¯åˆ†æ®µçš„å¥å­
        """
        try:
            doc = docx.Document(file_path)
            full_text = []
            current_line = ""
            
            for para in doc.paragraphs:
                text = para.text.strip()
                
                if not text:
                    if current_line:  # é‡åˆ°ç©ºè¡Œæ—¶æäº¤å½“å‰è¡Œ
                        full_text.append(current_line)
                        current_line = ""
                    continue
                    
                # åˆ¤æ–­æ˜¯å¦æ˜¯å¥å­ç»“å°¾ï¼ˆä¸­æ–‡æ ‡ç‚¹ï¼‰
                if text.endswith(('ã€‚', 'ï¼›', 'ï¼', 'ï¼Ÿ', 'ï¼‰', 'ã€', '.', ';', '!', '?')):
                    if current_line:
                        full_text.append(current_line + text)
                        current_line = ""
                    else:
                        full_text.append(text)
                else:
                    current_line += text
            
            # æ·»åŠ æœ€åæœªå®Œæˆçš„è¡Œ
            if current_line:
                full_text.append(current_line)
                
            return "\n".join(full_text)
        except Exception as e:
            raise ValueError(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")

    def extract_fields(self, text: str) -> dict:
        """ç²¾å‡†å­—æ®µæå–
        ä¸“æ³¨äºæå–äº§å“æè¿°ã€é¢„æœŸç”¨é€”å’Œå“åä¸‰ä¸ªå…³é”®å­—æ®µ
        """
        # é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼
        text = re.sub(r'[:ï¼š]\s*', ': ', text)  # ç»Ÿä¸€åˆ†éš”ç¬¦æ ¼å¼
        
        # å®šä¹‰å¤šçº§åŒ¹é…æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        patterns = {
            "desc": [
                r"äº§å“æè¿°[:ï¼š]\s*(.+?)(?=\n|$)",  # å¸¦æ ‡ç­¾æ ¼å¼
                r"ã€äº§å“æè¿°ã€‘(.+?)(?=ã€|$)",      # æ–¹æ‹¬å·æ ¼å¼
                r"æè¿°[:ï¼š]\s*(.+?)(?=\n|$)",     # ç®€ç•¥æ ‡ç­¾
                r"(?<=äº§å“æ¦‚è¿°[:ï¼š]).+?(?=\n|$)"  # åå‘æ–­è¨€
            ],
            "intended_use": [
                r"é¢„æœŸç”¨é€”[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ã€é¢„æœŸç”¨é€”ã€‘(.+?)(?=ã€|$)",
                r"ç”¨é€”[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ç”¨äº(.+?)(?=\n|$)",             # æ— æ ‡ç­¾æ ¼å¼
                r"é€‚ç”¨èŒƒå›´[:ï¼š]\s*(.+?)(?=\n|$)"
            ],
            "name": [
                r"å“å[:ï¼š]\s*(.+?)(?=\n|$)",
                r"ã€äº§å“åç§°ã€‘(.+?)(?=ã€|$)",
                r"åç§°[:ï¼š]\s*(.+?)(?=\n|$)",
                r"äº§å“å[:ï¼š]\s*(.+?)(?=\n|$)",
                r"æ³¨å†Œåç§°[:ï¼š]\s*(.+?)(?=\n|$)"
            ]
        }
        
        result = {}
        for field, field_patterns in patterns.items():
            # å°è¯•æ‰€æœ‰æ¨¡å¼ç›´åˆ°åŒ¹é…æˆåŠŸ
            for pattern in field_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    extracted = match.group(1).strip()
                    # ç®€å•æ¸…ç†æå–ç»“æœ
                    extracted = re.sub(r'^[:ï¼š\s]+|[:ï¼š\s]+$', '', extracted)
                    result[field] = extracted
                    break
            else:
                result[field] = ""
        
        # æ™ºèƒ½å›é€€ï¼šå¦‚æœæ ‡å‡†æ¨¡å¼æ²¡æ‰¾åˆ°ï¼Œå°è¯•é¡ºåºæå–
        if not all(result.values()):
            lines = [line for line in text.split('\n') if line.strip()]
            if len(lines) >= 3:
                # æ™ºèƒ½åˆ†é…ï¼šæœ€é•¿çš„é€šå¸¸æ˜¯æè¿°ï¼ŒåŒ…å«"ç”¨äº"çš„æ˜¯ç”¨é€”ï¼Œæœ€çŸ­çš„æ˜¯åç§°
                lines_sorted = sorted(lines, key=len)
                if not result["desc"]:
                    result["desc"] = lines_sorted[-1]  # æœ€é•¿çš„
                if not result["intended_use"] and "ç”¨äº" in text:
                    for line in lines:
                        if "ç”¨äº" in line:
                            result["intended_use"] = line
                            break
                if not result["name"]:
                    result["name"] = lines_sorted[0]  # æœ€çŸ­çš„
        
        # æœ€ç»ˆéªŒè¯å’Œæ¸…ç†
        result = {k: v.strip() for k, v in result.items()}
        
        # éªŒè¯å­—æ®µå®Œæ•´æ€§
        missing = [k for k, v in result.items() if not v]
        if missing:
            raise ValueError(f"æ–‡æ¡£ä¸­ç¼ºå°‘ä»¥ä¸‹å¿…è¦å­—æ®µ: {', '.join(missing)}")
        
        return result

    def retrieve_candidates(self, query: dict, top_k: int = 3) -> List[Dict]:
        """è¯­ä¹‰æ£€ç´¢æœ€ç›¸ä¼¼çš„å€™é€‰æ¡ç›®"""
        if not self.knowledge_base or self.embeddings is None:
            raise RuntimeError("çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_text = f"{query['name']}ã€‚{query['desc']}ã€‚ç”¨äº{query['intended_use']}"
        query_embedding = self.embedding_model.encode(
            query_text, 
            convert_to_tensor=True,
            normalize_embeddings=True
        )
        
        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        cos_scores = util.pytorch_cos_sim(query_embedding, self.embeddings)[0]
        top_results = torch.topk(cos_scores, k=top_k)
        
        # ç»„åˆç»“æœ
        results = []
        for score, idx in zip(top_results[0], top_results[1]):
            row = self.knowledge_base[idx]
            results.append({
                "row_id": row["row_id"],
                "desc": row["desc"],
                "intended_use": row["intended_use"],
                "name": row["name"],
                "grade": row["grade"],
                "similarity": float(score)  # è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
            })
        
        return results

    def generate_prompt(self, device_info: dict, candidates: list) -> str:
        """ç”Ÿæˆç»™LLMçš„æç¤ºè¯"""
        candidate_str = "\n".join(
            f"ã€æ¡ç›®#{c['row_id']}ã€‘\n"
            f"æè¿°: {c['desc']}\n"
            f"ç”¨é€”: {c['intended_use']}\n"
            f"åç§°: {c['name']}\n"
            f"ç±»åˆ«: {c['grade']}ç±»\n"
            f"è¯­ä¹‰ç›¸ä¼¼åº¦: {c['similarity']:.2f}\n"
            for c in candidates
        )
        
        return f"""è¯·å¯¹ä»¥ä¸‹åŒ»ç–—å™¨æ¢°è¿›è¡Œåˆ†ç±»åˆ†æï¼š

===== å¾…åˆ†ç±»è®¾å¤‡ =====
æè¿°: {device_info['desc']}
ç”¨é€”: {device_info['intended_use']}
åç§°: {device_info['name']}

===== çŸ¥è¯†åº“å€™é€‰æ¡ç›® =====
{candidate_str}

è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¿”å›JSONæ ¼å¼ç»“æœï¼Œå¿…é¡»åŒ…å«ï¼š
1. æ˜ç¡®çš„åˆ†ç±»ç­‰çº§ï¼ˆI/II/IIIï¼‰
2. ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
3. è¯¦ç»†çš„åˆ†ç±»ä¾æ®
4. åŒ¹é…çš„æ¡ç›®IDï¼ˆå¦‚æ— å¯å†™-1ï¼‰
5. éœ€è¦è¡¥å……çš„ä¿¡æ¯ï¼ˆå¦‚æ— å¯å†™ç©ºåˆ—è¡¨ï¼‰"""

    def process_document(self, file_path: str) -> dict:
        """å¤„ç†æ–‡æ¡£çš„ä¸»æµç¨‹"""
        try:
            # 1. è§£ææ–‡æ¡£
            text = self.parse_document(file_path)
            device_info = self.extract_fields(text)
            
            # 2. è¯­ä¹‰æ£€ç´¢å€™é€‰
            candidates = self.retrieve_candidates(device_info)
            
            # 3. ç”ŸæˆPromptå¹¶è°ƒç”¨API
            prompt = self.generate_prompt(device_info, candidates)
            api_response = self.call_deepseek_api(prompt)
            
            # 4. æå–JSONç»“æœ
            json_match = re.search(r"```json\n(.+?)\n```", api_response, re.DOTALL)
            if not json_match:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONå“åº”")
            
            result = json.loads(json_match.group(1))
            
            # éªŒè¯ç»“æœæ ¼å¼
            required_fields = ["classification", "confidence", "rationale"]
            if not all(field in result for field in required_fields):
                raise ValueError("APIè¿”å›ç¼ºå°‘å¿…è¦å­—æ®µ")
            
            return result
            
        except Exception as e:
            return {
                "error": str(e),
                "classification": "æœªçŸ¥",
                "confidence": 0,
                "rationale": "å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"
            }

def format_output(result: dict) -> str:
    """æ ¼å¼åŒ–è¾“å‡ºç»“æœ"""
    if "error" in result:
        return f"âŒ å¤„ç†é”™è¯¯: {result['error']}"
    
    # åˆ†ç±»ç­‰çº§æ˜ å°„
    class_map = {
        "I": "â… ç±»ï¼ˆä½é£é™©ï¼‰",
        "II": "â…¡ç±»ï¼ˆä¸­é£é™©ï¼‰",
        "III": "â…¢ç±»ï¼ˆé«˜é£é™©ï¼‰",
        "unknown": "æœªçŸ¥ç±»åˆ«"
    }
    
    classification = class_map.get(result["classification"].upper(), class_map["unknown"])
    confidence = f"{result['confidence']*100:.1f}%" if 'confidence' in result else "æœªçŸ¥"
    matched_id = result.get("matched_id", "æ— ")
    rationale = result.get("rationale", "æ— è¯´æ˜")
    
    # æ„å»ºMarkdownè¾“å‡º
    output = f"""## ğŸ¥ åŒ»ç–—å™¨æ¢°åˆ†ç±»ç»“æœ

**ğŸ” åˆ†ç±»ç­‰çº§**: {classification}  
**ğŸ“Š ç½®ä¿¡åº¦**: {confidence}  
**ğŸ”— åŒ¹é…æ¡ç›®ID**: {matched_id}

### ğŸ“ åˆ†ç±»ä¾æ®
{rationale}
"""
    
    # æ·»åŠ è¡¥å……ä¿¡æ¯æç¤º
    if result.get("missing_info"):
        output += f"\n\nâš ï¸ **éœ€è¦è¡¥å……çš„ä¿¡æ¯**: {', '.join(result['missing_info'])}"
    
    # æ·»åŠ å…è´£å£°æ˜
    output += "\n\n---\n*æ³¨ï¼šæœ¬ç»“æœåŸºäºAIåˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚æ­£å¼åˆ†ç±»éœ€ä»¥ç›‘ç®¡éƒ¨é—¨è®¤å®šä¸ºå‡†ã€‚*"
    
    return output

# åˆå§‹åŒ–åˆ†ç±»å™¨
classifier = MedicalDeviceClassifier()

with gr.Blocks(title="åŒ»ç–—å™¨æ¢°æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""<h1 align="center">ğŸ¥ åŒ»ç–—å™¨æ¢°åˆ†ç±»ç³»ç»Ÿï¼ˆæœ¬åœ°æ¨¡å‹ç‰ˆï¼‰</h1>""")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
            file_input = gr.File(
                label="ä¸Šä¼ åŒ»ç–—å™¨æ¢°æ–‡æ¡£",
                file_types=[".docx"],
                type="filepath"
            )
            submit_btn = gr.Button("å¼€å§‹åˆ†ç±»", variant="primary")
            
            gr.Markdown("### ğŸ“ æ–‡æ¡£æ ¼å¼è¦æ±‚")
            gr.Markdown("""
            è¯·ç¡®ä¿æ–‡æ¡£åŒ…å«ä»¥ä¸‹å­—æ®µï¼ˆä¸­è‹±æ–‡å‡å¯ï¼‰ï¼š
            - **äº§å“æè¿°**: å™¨æ¢°çš„åŠŸèƒ½å’Œç‰¹å¾
            - **é¢„æœŸç”¨é€”**: ä¸´åºŠç”¨é€”å’Œä½¿ç”¨æ–¹æ³•
            - **å“å**: äº§å“æ³¨å†Œåç§°
            
            ç¤ºä¾‹æ ¼å¼ï¼š
            ```
            äº§å“æè¿°: é«˜é¢‘å°„é¢‘æ‰‹æœ¯ç³»ç»Ÿ
            é¢„æœŸç”¨é€”: ç”¨äºå¤–ç§‘æ‰‹æœ¯ä¸­çš„ç»„ç»‡åˆ‡å‰²å’Œå‡è¡€
            å“å: é«˜é¢‘ç”µåˆ€
            ```
            """)
        
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ“Š åˆ†ç±»ç»“æœ")
            output = gr.Markdown(
                label="åˆ†æç»“æœ",
                value="ç­‰å¾…åˆ†æ...",
                show_copy_button=True
            )
    
    # å¤„ç†é€»è¾‘
    submit_btn.click(
        fn=lambda f: format_output(classifier.process_document(f)),
        inputs=file_input,
        outputs=output
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7862,
        show_error=True
    )